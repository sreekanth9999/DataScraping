{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The code below is an automated script to scrape the entire data from EAI Basesite and store it in an AWS S3 Database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib3.request\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import boto3\n",
    "from botocore.client import Config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages mostly used are beautifulSoup and Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this code hits the base site and gets to know how many rows are in the table sych that we can re-run the code \n",
    "#whenever the data gets updated \n",
    "\n",
    "outerlink = urllib.request.urlopen('https://openei.org/apps/USURDB/')\n",
    "outer_data_link = outerlink.read()\n",
    "outer_data_soup = BeautifulSoup(outer_data_link)\n",
    "no_of_iter=outer_data_soup.find(\"div\", {\"id\": \"urdbUtilityInfoContent2\"}).text.split()[1]\n",
    "iterations=int(no_of_iter)/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "round(iterations)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 1.The code below runs in a iterative for-loop for number of pages in the EAI website table. Using beatifulSoup functions it scrapes the data in Main Page and from the link provided in the each row it scrapes the Basic Information, Demand, Energy and Fixed Charges data of the corresponding row.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Once it scrapes the required data it appends to a pandas Dataframe row "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3.Have taken care of the errors that may occur while in loop using try-catch blocks  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datafile = pd.DataFrame()\n",
    "counter = 0\n",
    "\n",
    "for page_rank in range(1, round(iterations)+1):\n",
    "\n",
    "    try:\n",
    "        counter = counter + 1\n",
    "        print (counter)\n",
    "        link = \"https://openei.org/apps/USURDB/?page=\" + str(page_rank) + \"&rpp=100\"\n",
    "        wp = urllib.request.urlopen(link)\n",
    "        pw = wp.read()\n",
    "        soup = BeautifulSoup(pw)\n",
    "        right_table = soup.find('table', class_=\"table table-condensed table-bordered table-striped table-hover\")\n",
    "        A = []\n",
    "        B = []\n",
    "        C = []\n",
    "        D = []\n",
    "        E = []\n",
    "        F = []\n",
    "        G = []\n",
    "        H = []\n",
    "        I = []\n",
    "        J = []\n",
    "        K = []\n",
    "        L = []\n",
    "        M = []\n",
    "        N = []\n",
    "        O = []\n",
    "        P = []\n",
    "        Q = []\n",
    "        R = []\n",
    "        S = []\n",
    "        T = []\n",
    "        U = []\n",
    "        V = []\n",
    "        W = []\n",
    "        X = []\n",
    "        A4 = []\n",
    "        B4 = []\n",
    "        C4 = []\n",
    "        D4 = []\n",
    "\n",
    "        A3 = []\n",
    "        B3 = []\n",
    "\n",
    "        A2 = []\n",
    "        for row in right_table.findAll(\"tr\"):\n",
    "            cells = row.findAll('td')\n",
    "            A.append(cells[0].find(text=True))\n",
    "            B.append(cells[1].find(text=True))\n",
    "            C.append(cells[2].find(text=True))\n",
    "            D.append(cells[3].find(text=True))\n",
    "            E.append(cells[4].find(text=True))\n",
    "            F.append(cells[5].find(text=True))\n",
    "            G.append(cells[6].find(text=True))\n",
    "            H.append(cells[7].find(text=True))\n",
    "            all_links = cells[8].findAll('a')\n",
    "            inner_link_deep = \"\"\n",
    "\n",
    "            for link in all_links:\n",
    "                inner_link = link.get('href')\n",
    "                inner_link = \"https://openei.org/\" + inner_link\n",
    "                inner_link_deep = inner_link\n",
    "\n",
    "                ## Scraping for first page Starts\n",
    "\n",
    "                inner_link_deep_1 = inner_link_deep + '#1__Basic_Information'\n",
    "                wp1 = urllib.request.urlopen(inner_link_deep_1)\n",
    "                pw1 = wp1.read()\n",
    "                soup1 = BeautifulSoup(pw1)\n",
    "                right_table_1 = soup1.find('dl', class_=\"dl-horizontal\")\n",
    "                ead_id = right_table_1.find_all('dd')[1].text\n",
    "                I.append(ead_id)\n",
    "                description = right_table_1.find_all('dd')[9].text\n",
    "                J.append(description)\n",
    "                source_and_reference = right_table_1.find_all('dd')[10].text\n",
    "                K.append(source_and_reference)\n",
    "                source_parent = right_table_1.find_all('dd')[11].text\n",
    "                L.append(source_parent)\n",
    "                Revision_History = soup1.findAll('fieldset')[8]\n",
    "                M.append(Revision_History.findAll('dl')[0].get_text())\n",
    "                demand_factors = soup1.findAll('fieldset')[3]\n",
    "                demand_inner = demand_factors.findAll('dl')\n",
    "                N.append(''.join([x.findAll('dd')[0].text for x in demand_inner]))\n",
    "                O.append(''.join([x.findAll('dd')[1].text for x in demand_inner]))\n",
    "                P.append(''.join([x.findAll('dd')[2].text for x in demand_inner]))\n",
    "                Q.append(''.join([x.findAll('dd')[3].text for x in demand_inner]))\n",
    "                Energy_factors = soup1.findAll('fieldset')[4]\n",
    "                energy_inner = Energy_factors.findAll('dl')\n",
    "                R.append(''.join([x.findAll('dd')[0].text for x in energy_inner]))\n",
    "                S.append(''.join([x.findAll('dd')[1].text for x in energy_inner]))\n",
    "                T.append(''.join([x.findAll('dd')[2].text for x in energy_inner]))\n",
    "                Service_Voltage = soup1.findAll('fieldset')[5]\n",
    "                service_inner = Service_Voltage.findAll('dl')\n",
    "                U.append(''.join([x.findAll('dd')[0].text for x in service_inner]))\n",
    "                V.append(''.join([x.findAll('dd')[1].text for x in service_inner]))\n",
    "                Character_of_Service = soup1.findAll('fieldset')[6]\n",
    "                Character_inner = Character_of_Service.findAll('dl')\n",
    "                W.append(''.join([x.findAll('dd')[0].text for x in Character_inner]))\n",
    "                X.append(''.join([x.findAll('dd')[1].text for x in Character_inner]))\n",
    "\n",
    "                # Scraping for Fixed Charges page\n",
    "\n",
    "                inner_link_deep_fixed_charges = inner_link_deep\n",
    "                wp4 = urllib.request.urlopen(inner_link_deep_fixed_charges)\n",
    "                pw4 = wp4.read()\n",
    "                soup4 = BeautifulSoup(pw4)\n",
    "                inner_fixed_charges = soup4.findAll('fieldset')[19]\n",
    "                fixed_charges_inner = inner_fixed_charges.findAll('dl')\n",
    "                A4.append(''.join([x.findAll('dd')[0].text for x in fixed_charges_inner]))\n",
    "                B4.append(''.join([x.findAll('dd')[1].text for x in fixed_charges_inner]))\n",
    "                C4.append(''.join([x.findAll('dd')[2].text for x in fixed_charges_inner]))\n",
    "                D4.append(''.join([x.findAll('dd')[3].text for x in fixed_charges_inner]))\n",
    "\n",
    "                # Scraping for Energy page::\n",
    "                Energy_comments = soup4.findAll('fieldset')[18]\n",
    "                Energy_comments_data = Energy_comments.find('span').text\n",
    "                A3.append(Energy_comments_data)\n",
    "\n",
    "                energy_table_data = soup1.findAll('fieldset')[15]\n",
    "                list_energy1 = []\n",
    "                list_energy2 = []\n",
    "                energy_table_header = energy_table_data.findAll('div', class_='strux_view_cell tier_col')\n",
    "                energy_table_body = energy_table_data.findAll('div', class_='strux_view_row')\n",
    "                list_energy2 = [data.text.strip() for data in energy_table_header]\n",
    "                list_energy2.append([data.text.split('\\n') for data in energy_table_body])\n",
    "                # B3.append(list_energy1)\n",
    "                B3.append(list_energy2)\n",
    "\n",
    "                # Scraping from Demand Page::\n",
    "\n",
    "                demand_table = soup1.findAll('fieldset')[9]\n",
    "                list_demand1 = []\n",
    "                list_demand2 = []\n",
    "                demand_table_header1 = demand_table.findAll('header')[0]\n",
    "                demand_table_header = demand_table_header1.findAll('div', class_='strux_view_cell')\n",
    "                demand_table_body = demand_table.findAll('div', class_='strux_view_row')\n",
    "\n",
    "                list_demand1 = [data.text.strip() for data in demand_table_header]\n",
    "                list_demand1.append([data.text.split('\\n') for data in demand_table_body])\n",
    "                A2.append(list_demand1)\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "        df['Utility Name'] = A\n",
    "        df['Rate Name'] = B\n",
    "        df['Sector'] = C\n",
    "        df['Service Type'] = D\n",
    "        df['Approved'] = E\n",
    "        df['Effective Date'] = F\n",
    "        df['End Date'] = G\n",
    "        df['Latest Update'] = H\n",
    "        df['EAI ID'] = I\n",
    "        df['Description'] = J\n",
    "        df['Source or Reference'] = K\n",
    "        df['Source Parent'] = L\n",
    "        df['Revision History'] = M\n",
    "        df['Demand_Minimum'] = N\n",
    "        df['Demand_Maximim'] = O\n",
    "        df['Demand_Units'] = P\n",
    "        df['Demand_History(months)'] = Q\n",
    "        df['Energy Minimum(kWh)'] = R\n",
    "        df['Energy Maximum(kWh)'] = S\n",
    "        df['Energy_History (months)'] = T\n",
    "        df['Service Minimum(V)'] = U\n",
    "        df['Service Maximim(V)'] = V\n",
    "        df['Character of Service Voltage Category'] = W\n",
    "        df['Character Of Service Phase Wiring'] = X\n",
    "        df['Fixed Charge Units'] = A4\n",
    "        df['Fixed Charge (First Meter)'] = B4\n",
    "        df['Fixed Charge (Each Additional Meter)'] = C4\n",
    "        df['Minimum Charge_fixed_charges'] = D4\n",
    "        df['Energy Comments'] = A3\n",
    "        df['Tiered Energy Usage Charge Structure'] = B3\n",
    "        df['Seasonal/Monthly Demand Charge Structure'] = A2\n",
    "\n",
    "        datafile = pd.concat([datafile, df], axis=0)\n",
    "\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "        print (\"error in\",counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(datafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datafile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datafile.to_csv('C:/Users/sreej/Desktop/datafile1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datafile.to_csv('C:/Users/sreej/OneDrive/datafile1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Finally created an S3 database and stored the scrapped data into the bucket.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.client import Config\n",
    "\n",
    "ACCESS_KEY_ID = ''   ## replace with your access key\n",
    "ACCESS_SECRET_KEY = ''  ## replace with your ksy\n",
    "BUCKET_NAME = 'incretechchallengesree'\n",
    "\n",
    "data = open('C:/Users/sreej/Desktop/datafile.csv', 'rb')\n",
    "\n",
    "s3 = boto3.resource(\n",
    "    's3',\n",
    "    aws_access_key_id=ACCESS_KEY_ID,\n",
    "    aws_secret_access_key=ACCESS_SECRET_KEY,\n",
    "    config=Config(signature_version='s3v4')\n",
    ")\n",
    "s3.Bucket(BUCKET_NAME).put_object(Key='EAIBASESITE_SCRAPPING_TASK', Body=data)\n",
    "\n",
    "print (\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
